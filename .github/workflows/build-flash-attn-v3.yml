name: Build FlashAttention-3 for CUDA 12.8 / PyTorch 2.8 / Python 3.12
on:                      # run manually from the Actions tab
  workflow_dispatch:

jobs:
  build-wheel:
    runs-on: ubuntu-22.04        # GitHub-hosted runner (CPU-only is OK)

    steps:
    #---------------------------------------------------------------
    # 1. Check out FlashAttention source
    #---------------------------------------------------------------
    - name: Checkout FlashAttention
      uses: actions/checkout@v4
      with:
        repository: Dao-AILab/flash-attention
        path: flash-attention

    #---------------------------------------------------------------
    # 2. Add NVIDIA CUDA 12.x APT repo and install the toolkit
    #    (brings in nvcc + headers; no GPU required to compile)
    #---------------------------------------------------------------
    - name: Install CUDA 12.8 toolkit
      run: |
        sudo apt-get update
        # add NVIDIA signing key
        sudo mkdir -p /etc/apt/keyrings
        curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub \
          | sudo gpg --dearmor -o /etc/apt/keyrings/cuda-archive-keyring.gpg
        # add repository
        echo "deb [signed-by=/etc/apt/keyrings/cuda-archive-keyring.gpg] \
          https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /" \
          | sudo tee /etc/apt/sources.list.d/cuda.list
        sudo apt-get update
        sudo apt-get -y install cuda-toolkit-12-8

    #---------------------------------------------------------------
    # 3. Set up Python 3.12 and matching PyTorch 2.8 + cu128
    #---------------------------------------------------------------
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install PyTorch 2.8 + CUDA 12.8 build
      run: |
        python -m pip install --upgrade pip
        pip install torch==2.8.0+cu128 torchvision==0.15.0+cu128 \
          --extra-index-url https://download.pytorch.org/whl/cu128

    #---------------------------------------------------------------
    # 4. Build FlashAttention-3 wheel
    #---------------------------------------------------------------
    - name: Install build dependencies
      run: pip install setuptools wheel ninja

    - name: Build wheel
      working-directory: flash-attention
      run: |
        git fetch --tags
        git checkout v3.0.0           # change if a newer v3 tag exists
        pip wheel . --no-deps -w dist/

    # Optional: show what we produced
    - name: List dist/ contents
      run: ls -lh flash-attention/dist/

    #---------------------------------------------------------------
    # 5. Upload wheel artifact (v4 action, survives 2025 cutoff)
    #---------------------------------------------------------------
    - name: Upload wheel
      uses: actions/upload-artifact@v4
      with:
        name: flash-attn-v3-cu128-torch2.8-py312
        path: flash-attention/dist/*.whl     # ‚Üê glob must match a file
        if-no-files-found: error
        retention-days: 14
