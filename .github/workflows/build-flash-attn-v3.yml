name: Build FlashAttention v3 (CUDA12.8 + PyTorch2.8 + Python3.12)

on:
  workflow_dispatch:   # manual trigger

jobs:
  build-wheel:
    runs-on: ubuntu-22.04          # GitHub-hosted VM (CPU only)

    steps:
    # -------------------------------------------------------------
    # 1. Grab FlashAttention source code
    # -------------------------------------------------------------
    - name: Checkout FlashAttention
      uses: actions/checkout@v4
      with:
        repository: Dao-AILab/flash-attention
        path: flash-attention

    # -------------------------------------------------------------
    # 2. Install CUDA 12.8 toolkit (for NVCC)
    #    → *compilation* works even on CPU runners;
    #      you’ll still want a GPU to *use* the wheel.
    # -------------------------------------------------------------
    - name: Set up CUDA 12.8
      run: |
        sudo apt-get update
        wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda-repo-ubuntu2204-12-8-local_12.8.0-1_amd64.deb
        sudo dpkg -i cuda-repo-ubuntu2204-12-8-local_12.8.0-1_amd64.deb
        sudo cp /var/cuda-repo-ubuntu2204-12-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
        sudo apt-get update
        sudo apt-get -y install cuda-toolkit-12-8

    # -------------------------------------------------------------
    # 3. Choose Python 3.12
    # -------------------------------------------------------------
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    # -------------------------------------------------------------
    # 4. Install matching PyTorch (2.8 + cu128 build)
    # -------------------------------------------------------------
    - name: Install PyTorch 2.8 (cu128)
      run: |
        python -m pip install --upgrade pip
        pip install torch==2.8.0+cu128 torchvision==0.15.0+cu128 \
          --extra-index-url https://download.pytorch.org/whl/cu128

    # -------------------------------------------------------------
    # 5. Build FlashAttention-3 wheel
    # -------------------------------------------------------------
    - name: Install build deps
      run: pip install setuptools wheel ninja

    - name: Build wheel
      working-directory: flash-attention
      run: |
        git fetch --tags
        git checkout v3.0.0          # <-- change if a newer tag exists
        pip wheel . --no-deps -w dist/

    # (Optional) show what we built
    - name: List contents of dist/
      run: ls -lh flash-attention/dist/

    # -------------------------------------------------------------
    # 6. Upload wheel as an artifact (v4 action!)
    # -------------------------------------------------------------
    - name: Upload wheel artifact
      uses: actions/upload-artifact@v4
      with:
        name: flash-attn-v3-cu128-torch2.8-py312
        path: flash-attention/dist/*.whl
        if-no-files-found: error      # fail fast if the glob is empty
        retention-days: 7             # keep the wheel for a week
